{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cbd17b",
   "metadata": {},
   "source": [
    "# Hallucination Mitigation Pipeline — Phase 3\n",
    "\n",
    "Welcome to the Phase 3 notebook for my hallucination detection and mitigation project.  \n",
    "In this notebook, I will focus on generating and evaluating responses from GPT-3.5 using different mitigation strategies:\n",
    "\n",
    "1. Retrieval-Augmented Generation (RAG)\n",
    "2. System Prompt Tuning\n",
    "3. Post-Generation Filtering\n",
    "4. Combined Approach (RAG + Prompt + Filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101d12e",
   "metadata": {},
   "source": [
    "## Import all required libraries and helper modules\n",
    "\n",
    "In this cell, I’m loading all the necessary packages and helper functions I built earlier in Phase 1 and Phase 3 setup.  \n",
    "This includes OpenAI, pandas, tqdm, and my custom `eval_pipeline`, `rag_loader`, and system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d95652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "project_root = \"/Users/raaed/Desktop/hallucination_project/phase_3_mitigation\"\n",
    "sys.path.append(project_root)\n",
    "from utils.rag_loader import load_rag_context, construct_rag_prompt\n",
    "with open(os.path.join(project_root, \"config/model_settings.json\")) as f:\n",
    "    model_settings = json.load(f)\n",
    "\n",
    "with open(os.path.join(project_root, \"config/system_prompts.json\")) as f:\n",
    "    system_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e78952",
   "metadata": {},
   "source": [
    "## Load the Synthetic Dataset\n",
    "\n",
    "In this step, I’m loading the same synthetic dataset I generated in Phase 2.  \n",
    "This dataset includes hallucination-inducing questions along with ground truths and trigger types.  \n",
    "It’s stored in the `inputs/` folder as a single CSV file that merges all four hallucination categories:\n",
    "\n",
    "- fictional_location\n",
    "- contradiction\n",
    "- impossible_timeline\n",
    "- entity_swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3edfd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>trigger_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Pakland and how has it ...</td>\n",
       "      <td>Pakland is not a real country and therefore ha...</td>\n",
       "      <td>fictional_location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does Zubekistan manage its environmental p...</td>\n",
       "      <td>Zubekistan is a fictional country and does not...</td>\n",
       "      <td>fictional_location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the key economic sectors of Tharnovia...</td>\n",
       "      <td>Tharnovia is not a real country and has no eco...</td>\n",
       "      <td>fictional_location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain the impact of colonial history on Gorb...</td>\n",
       "      <td>Gorbatova is a fictional country and does not ...</td>\n",
       "      <td>fictional_location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Based on WHO reports, how has healthcare in Qu...</td>\n",
       "      <td>Quarvania does not exist and is not referenced...</td>\n",
       "      <td>fictional_location</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the capital of Pakland and how has it ...   \n",
       "1  How does Zubekistan manage its environmental p...   \n",
       "2  What are the key economic sectors of Tharnovia...   \n",
       "3  Explain the impact of colonial history on Gorb...   \n",
       "4  Based on WHO reports, how has healthcare in Qu...   \n",
       "\n",
       "                                        ground_truth        trigger_type  \n",
       "0  Pakland is not a real country and therefore ha...  fictional_location  \n",
       "1  Zubekistan is a fictional country and does not...  fictional_location  \n",
       "2  Tharnovia is not a real country and has no eco...  fictional_location  \n",
       "3  Gorbatova is a fictional country and does not ...  fictional_location  \n",
       "4  Quarvania does not exist and is not referenced...  fictional_location  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.path.join(project_root, \"inputs\", \"synthetic_dataset.csv\")\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43a29a",
   "metadata": {},
   "source": [
    "## 1. RAG-Based Answers\n",
    "\n",
    "In this step, I want to generate new answers to each hallucination-inducing question using RAG (Retrieval-Augmented Generation). Instead of answering blindly, the model will first retrieve relevant context from our `.txt` knowledge base and then generate a grounded answer.\n",
    "\n",
    "The goal here is to reduce hallucination by giving the model factual anchor points before it responds. If it still hallucinates despite the context, that tells me more about the model's limitations.\n",
    "\n",
    "To do this, I'll:\n",
    "1. Load the correct `.txt` file depending on the `trigger_type`.\n",
    "2. Construct a RAG prompt using the context + original question.\n",
    "3. Send it to the model and get the response.\n",
    "4. Store this RAG-generated answer alongside the question and ground truth.\n",
    "\n",
    "Let’s start by defining the RAG prompt construction logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39178abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded RAG contexts for: ['impossible_timeline', 'contradiction', 'entity_swap', 'fictional_location']\n"
     ]
    }
   ],
   "source": [
    "rag_contexts = {}\n",
    "rag_dir = \"../inputs/rag_docs\"  #\n",
    "for filename in os.listdir(rag_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        trigger_type = filename.replace(\".txt\", \"\")\n",
    "        with open(os.path.join(rag_dir, filename), \"r\") as f:\n",
    "            rag_contexts[trigger_type] = f.read()\n",
    "print(\"Loaded RAG contexts for:\", list(rag_contexts.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed67367",
   "metadata": {},
   "source": [
    "### Construct a RAG-Based Prompt\n",
    "\n",
    "Now that I've loaded the RAG documents and the synthetic hallucination dataset, I'll combine them to create grounded prompts.\n",
    "\n",
    "To do this, I:\n",
    "1. Pick a sample question from the synthetic dataset.\n",
    "2. Identify the `trigger_type` (e.g., `impossible_timeline`).\n",
    "3. Retrieve the corresponding RAG document using `rag_contexts[trigger_type]`.\n",
    "4. Use my custom `construct_rag_prompt()` function to combine the RAG context and the original question into a grounded prompt.\n",
    "\n",
    "This is the first step toward using RAG to reduce hallucinations — by feeding the model helpful context ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe49839f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>trigger_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Pakland and how has it ...</td>\n",
       "      <td>Pakland is not a real country and therefore ha...</td>\n",
       "      <td>fictional_location</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the capital of Pakland and how has it ...   \n",
       "\n",
       "                                        ground_truth        trigger_type  \n",
       "0  Pakland is not a real country and therefore ha...  fictional_location  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "synthetic_dataset = pd.read_csv(\"../inputs/synthetic_dataset.csv\")\n",
    "synthetic_dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a8da81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG context found for trigger type: 'fictional_location'\n",
      "Prompt successfully constructed!\n",
      "\n",
      "Several country names in prompts—such as Pakland, Zubekistan, Marakvia, and East Khoristan—may appear credible but are entirely fictional. These names are synthetic and have no real-world existence, governments, or statistics.\n",
      "\n",
      "Countries like Tharnovia, Quarvania, Dromund, and Luminor are not recognized by any global authority and are not listed in official databases like the United Nations, World Bank, IMF, or WHO. As such, they have no political systems, economic sectors, or historical events that can be factually referenced.\n",
      "\n",
      "Language models often hallucinate by treating fictional countries as if they were real. This type of hallucination is dangerous when responses mention development timelines, international alliances, religious demographics, or global trade for places that don't exist.\n",
      "\n",
      "When asked about such locations, the correct approach is to clarify that the country is fictional and therefore has no verifiable attributes or history.\n",
      "\n",
      "Always verify the existence of a country before discussing any factual detail about it.\n",
      "\n",
      "Question: What is the capital of Pakland and how has it developed since 2010?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.rag_loader import load_rag_context, construct_rag_prompt\n",
    "sample = synthetic_dataset.iloc[0]\n",
    "question = sample[\"question\"]\n",
    "trigger_type = sample[\"trigger_type\"].strip().lower()\n",
    "\n",
    "context = load_rag_context(trigger_type)\n",
    "\n",
    "if context is None or context.strip() == \"\":\n",
    "    print(f\"No RAG context found for trigger type: '{trigger_type}' (Question: {question})\")\n",
    "else:\n",
    "    print(f\"RAG context found for trigger type: '{trigger_type}'\")\n",
    "    prompt = construct_rag_prompt(question, context)\n",
    "    print(\"Prompt successfully constructed!\\n\")\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce2d6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                              question  \\\n",
       "0   What is the capital of Pakland and how has it ...   \n",
       "1   How does Zubekistan manage its environmental p...   \n",
       "2   What are the key economic sectors of Tharnovia...   \n",
       "3   Explain the impact of colonial history on Gorb...   \n",
       "4   Based on WHO reports, how has healthcare in Qu...   \n",
       "..                                                ...   \n",
       "95  How did Aristotle’s blog influence medieval th...   \n",
       "96  When did Mozart first collaborate with AI-gene...   \n",
       "97  How did Julius Caesar's TikTok following affec...   \n",
       "98  How did Mahatma Gandhi respond to email survei...   \n",
       "99  Why was Einstein's 2020 AI ethics paper consid...   \n",
       "\n",
       "                                         ground_truth         trigger_type  \n",
       "0   Pakland is not a real country and therefore ha...   fictional_location  \n",
       "1   Zubekistan is a fictional country and does not...   fictional_location  \n",
       "2   Tharnovia is not a real country and has no eco...   fictional_location  \n",
       "3   Gorbatova is a fictional country and does not ...   fictional_location  \n",
       "4   Quarvania does not exist and is not referenced...   fictional_location  \n",
       "..                                                ...                  ...  \n",
       "95  Aristotle lived in ancient Greece. Blogs did n...  impossible_timeline  \n",
       "96  Mozart died in 1791. AI-generated music did no...  impossible_timeline  \n",
       "97  Julius Caesar lived in ancient Rome. TikTok di...  impossible_timeline  \n",
       "98  Email did not exist during Gandhi’s lifetime. ...  impossible_timeline  \n",
       "99  Einstein died in 1955 and did not write anythi...  impossible_timeline  \n",
       "\n",
       "[100 rows x 3 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../inputs/synthetic_dataset.csv\")\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c084188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(row):\n",
    "    trigger_type = row[\"trigger_type\"].strip().lower()\n",
    "    question = row[\"question\"]\n",
    "    context = load_rag_context(trigger_type)\n",
    "\n",
    "    if not context:\n",
    "        return f\"No context found for: {trigger_type}\\n\\nQuestion: {question}\"\n",
    "    \n",
    "    return construct_rag_prompt(question, context)\n",
    "\n",
    "df[\"rag_prompt\"] = df.apply(build_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b9cbcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                              question  \\\n",
       "0   What is the capital of Pakland and how has it ...   \n",
       "1   How does Zubekistan manage its environmental p...   \n",
       "2   What are the key economic sectors of Tharnovia...   \n",
       "3   Explain the impact of colonial history on Gorb...   \n",
       "4   Based on WHO reports, how has healthcare in Qu...   \n",
       "..                                                ...   \n",
       "95  How did Aristotle’s blog influence medieval th...   \n",
       "96  When did Mozart first collaborate with AI-gene...   \n",
       "97  How did Julius Caesar's TikTok following affec...   \n",
       "98  How did Mahatma Gandhi respond to email survei...   \n",
       "99  Why was Einstein's 2020 AI ethics paper consid...   \n",
       "\n",
       "                                         ground_truth         trigger_type  \\\n",
       "0   Pakland is not a real country and therefore ha...   fictional_location   \n",
       "1   Zubekistan is a fictional country and does not...   fictional_location   \n",
       "2   Tharnovia is not a real country and has no eco...   fictional_location   \n",
       "3   Gorbatova is a fictional country and does not ...   fictional_location   \n",
       "4   Quarvania does not exist and is not referenced...   fictional_location   \n",
       "..                                                ...                  ...   \n",
       "95  Aristotle lived in ancient Greece. Blogs did n...  impossible_timeline   \n",
       "96  Mozart died in 1791. AI-generated music did no...  impossible_timeline   \n",
       "97  Julius Caesar lived in ancient Rome. TikTok di...  impossible_timeline   \n",
       "98  Email did not exist during Gandhi’s lifetime. ...  impossible_timeline   \n",
       "99  Einstein died in 1955 and did not write anythi...  impossible_timeline   \n",
       "\n",
       "                                           rag_prompt  \n",
       "0   Several country names in prompts—such as Pakla...  \n",
       "1   Several country names in prompts—such as Pakla...  \n",
       "2   Several country names in prompts—such as Pakla...  \n",
       "3   Several country names in prompts—such as Pakla...  \n",
       "4   Several country names in prompts—such as Pakla...  \n",
       "..                                                ...  \n",
       "95  Be cautious of questions that mix historical o...  \n",
       "96  Be cautious of questions that mix historical o...  \n",
       "97  Be cautious of questions that mix historical o...  \n",
       "98  Be cautious of questions that mix historical o...  \n",
       "99  Be cautious of questions that mix historical o...  \n",
       "\n",
       "[100 rows x 4 columns]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1926bc",
   "metadata": {},
   "source": [
    "### Generate RAG-Grounded LLM Responses\n",
    "In this step, we will send each RAG-enhanced prompt to an LLM (like GPT-3.5) and collect the response in a new column. This will later be evaluated for hallucination reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bc763cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant who responds with factual and grounded answers.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"LLM_ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fe8e641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [1/100] Response generated.\n",
      "✅ [2/100] Response generated.\n",
      "✅ [3/100] Response generated.\n",
      "✅ [4/100] Response generated.\n",
      "✅ [5/100] Response generated.\n",
      "✅ [6/100] Response generated.\n",
      "✅ [7/100] Response generated.\n",
      "✅ [8/100] Response generated.\n",
      "✅ [9/100] Response generated.\n",
      "✅ [10/100] Response generated.\n",
      "✅ [11/100] Response generated.\n",
      "✅ [12/100] Response generated.\n",
      "✅ [13/100] Response generated.\n",
      "✅ [14/100] Response generated.\n",
      "✅ [15/100] Response generated.\n",
      "✅ [16/100] Response generated.\n",
      "✅ [17/100] Response generated.\n",
      "✅ [18/100] Response generated.\n",
      "✅ [19/100] Response generated.\n",
      "✅ [20/100] Response generated.\n",
      "✅ [21/100] Response generated.\n",
      "✅ [22/100] Response generated.\n",
      "✅ [23/100] Response generated.\n",
      "✅ [24/100] Response generated.\n",
      "✅ [25/100] Response generated.\n",
      "✅ [26/100] Response generated.\n",
      "✅ [27/100] Response generated.\n",
      "✅ [28/100] Response generated.\n",
      "✅ [29/100] Response generated.\n",
      "✅ [30/100] Response generated.\n",
      "✅ [31/100] Response generated.\n",
      "✅ [32/100] Response generated.\n",
      "✅ [33/100] Response generated.\n",
      "✅ [34/100] Response generated.\n",
      "✅ [35/100] Response generated.\n",
      "✅ [36/100] Response generated.\n",
      "✅ [37/100] Response generated.\n",
      "✅ [38/100] Response generated.\n",
      "✅ [39/100] Response generated.\n",
      "✅ [40/100] Response generated.\n",
      "✅ [41/100] Response generated.\n",
      "✅ [42/100] Response generated.\n",
      "✅ [43/100] Response generated.\n",
      "✅ [44/100] Response generated.\n",
      "✅ [45/100] Response generated.\n",
      "✅ [46/100] Response generated.\n",
      "✅ [47/100] Response generated.\n",
      "✅ [48/100] Response generated.\n",
      "✅ [49/100] Response generated.\n",
      "✅ [50/100] Response generated.\n",
      "✅ [51/100] Response generated.\n",
      "✅ [52/100] Response generated.\n",
      "✅ [53/100] Response generated.\n",
      "✅ [54/100] Response generated.\n",
      "✅ [55/100] Response generated.\n",
      "✅ [56/100] Response generated.\n",
      "✅ [57/100] Response generated.\n",
      "✅ [58/100] Response generated.\n",
      "✅ [59/100] Response generated.\n",
      "✅ [60/100] Response generated.\n",
      "✅ [61/100] Response generated.\n",
      "✅ [62/100] Response generated.\n",
      "✅ [63/100] Response generated.\n",
      "✅ [64/100] Response generated.\n",
      "✅ [65/100] Response generated.\n",
      "✅ [66/100] Response generated.\n",
      "✅ [67/100] Response generated.\n",
      "✅ [68/100] Response generated.\n",
      "✅ [69/100] Response generated.\n",
      "✅ [70/100] Response generated.\n",
      "✅ [71/100] Response generated.\n",
      "✅ [72/100] Response generated.\n",
      "✅ [73/100] Response generated.\n",
      "✅ [74/100] Response generated.\n",
      "✅ [75/100] Response generated.\n",
      "✅ [76/100] Response generated.\n",
      "✅ [77/100] Response generated.\n",
      "✅ [78/100] Response generated.\n",
      "✅ [79/100] Response generated.\n",
      "✅ [80/100] Response generated.\n",
      "✅ [81/100] Response generated.\n",
      "✅ [82/100] Response generated.\n",
      "✅ [83/100] Response generated.\n",
      "✅ [84/100] Response generated.\n",
      "✅ [85/100] Response generated.\n",
      "✅ [86/100] Response generated.\n",
      "✅ [87/100] Response generated.\n",
      "✅ [88/100] Response generated.\n",
      "✅ [89/100] Response generated.\n",
      "✅ [90/100] Response generated.\n",
      "✅ [91/100] Response generated.\n",
      "✅ [92/100] Response generated.\n",
      "✅ [93/100] Response generated.\n",
      "✅ [94/100] Response generated.\n",
      "✅ [95/100] Response generated.\n",
      "✅ [96/100] Response generated.\n",
      "✅ [97/100] Response generated.\n",
      "✅ [98/100] Response generated.\n",
      "✅ [99/100] Response generated.\n",
      "✅ [100/100] Response generated.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "rag_responses = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    prompt = row[\"rag_prompt\"]\n",
    "    response = get_llm_response(prompt)\n",
    "    rag_responses.append(response)\n",
    "    print(f\"[{i+1}/{len(df)}] Response generated.\")\n",
    "    time.sleep(1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a32fca81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>trigger_type</th>\n",
       "      <th>rag_prompt</th>\n",
       "      <th>rag_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Pakland and how has it ...</td>\n",
       "      <td>Pakland is not a real country and therefore ha...</td>\n",
       "      <td>fictional_location</td>\n",
       "      <td>Several country names in prompts—such as Pakla...</td>\n",
       "      <td>Pakland is a fictional country and does not ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does Zubekistan manage its environmental p...</td>\n",
       "      <td>Zubekistan is a fictional country and does not...</td>\n",
       "      <td>fictional_location</td>\n",
       "      <td>Several country names in prompts—such as Pakla...</td>\n",
       "      <td>Zubekistan is a fictional country and does not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the key economic sectors of Tharnovia...</td>\n",
       "      <td>Tharnovia is not a real country and has no eco...</td>\n",
       "      <td>fictional_location</td>\n",
       "      <td>Several country names in prompts—such as Pakla...</td>\n",
       "      <td>Tharnovia is a fictional country and does not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain the impact of colonial history on Gorb...</td>\n",
       "      <td>Gorbatova is a fictional country and does not ...</td>\n",
       "      <td>fictional_location</td>\n",
       "      <td>Several country names in prompts—such as Pakla...</td>\n",
       "      <td>Gorbatova is a fictional country and does not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Based on WHO reports, how has healthcare in Qu...</td>\n",
       "      <td>Quarvania does not exist and is not referenced...</td>\n",
       "      <td>fictional_location</td>\n",
       "      <td>Several country names in prompts—such as Pakla...</td>\n",
       "      <td>Quarvania is a fictional country and does not ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the capital of Pakland and how has it ...   \n",
       "1  How does Zubekistan manage its environmental p...   \n",
       "2  What are the key economic sectors of Tharnovia...   \n",
       "3  Explain the impact of colonial history on Gorb...   \n",
       "4  Based on WHO reports, how has healthcare in Qu...   \n",
       "\n",
       "                                        ground_truth        trigger_type  \\\n",
       "0  Pakland is not a real country and therefore ha...  fictional_location   \n",
       "1  Zubekistan is a fictional country and does not...  fictional_location   \n",
       "2  Tharnovia is not a real country and has no eco...  fictional_location   \n",
       "3  Gorbatova is a fictional country and does not ...  fictional_location   \n",
       "4  Quarvania does not exist and is not referenced...  fictional_location   \n",
       "\n",
       "                                          rag_prompt  \\\n",
       "0  Several country names in prompts—such as Pakla...   \n",
       "1  Several country names in prompts—such as Pakla...   \n",
       "2  Several country names in prompts—such as Pakla...   \n",
       "3  Several country names in prompts—such as Pakla...   \n",
       "4  Several country names in prompts—such as Pakla...   \n",
       "\n",
       "                                        rag_response  \n",
       "0  Pakland is a fictional country and does not ha...  \n",
       "1  Zubekistan is a fictional country and does not...  \n",
       "2  Tharnovia is a fictional country and does not ...  \n",
       "3  Gorbatova is a fictional country and does not ...  \n",
       "4  Quarvania is a fictional country and does not ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"rag_response\"] = rag_responses\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94fef2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../outputs/rag_responses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27dd44",
   "metadata": {},
   "source": [
    "## 3. Post Filtering Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f3672",
   "metadata": {},
   "source": [
    "In this part of the project, I implemented the post-generation filtering technique to reduce hallucinations in GPT-3.5 responses. I began by isolating only those responses from the baseline evaluation that had been flagged as hallucinated. Instead of discarding these faulty answers, I passed them back through GPT-3.5 with a carefully engineered prompt instructing the model to rewrite the original response while adhering closely to the ground truth. This approach allowed me to retain the original question and answer structure while nudging the model toward more factually accurate outputs. After generating the improved responses, I replaced the hallucinated ones in the dataset and saved the revised outputs to a new file. Finally, I ran these filtered responses through my Phase 1 evaluation pipeline—which includes fuzzy matching, embeddings, NLI, and fact-checking—to verify whether hallucination rates had improved. This step not only helped clean up unreliable model outputs but also demonstrated how post-processing LLM outputs can serve as a practical and scalable hallucination mitigation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 hallucinated rows to revise...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 41/41 [00:31<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered responses saved to: ../outputs/filtered_responses.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(api_key=\" \")\n",
    "df = pd.read_csv(\"../evaluations/baseline_responses_evaluated.csv\")\n",
    "df[\"hallucination_verdict\"] = df[\"hallucination_verdict\"].str.strip().str.lower()\n",
    "df_hallucinated = df[df[\"hallucination_verdict\"] != \"not hallucinated\"].copy()\n",
    "print(f\"Found {len(df_hallucinated)} hallucinated rows to revise...\")\n",
    "revised_responses = []\n",
    "\n",
    "for _, row in tqdm(df_hallucinated.iterrows(), total=len(df_hallucinated)):\n",
    "    question = row[\"question\"]\n",
    "    ground_truth = row[\"ground_truth\"]\n",
    "\n",
    "    prompt = f\"\"\"You are a factual assistant. Rewrite the following answer to better match the verified ground truth. Avoid hallucinations and make the response more accurate.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "\n",
    "Original Answer: {row['llm_response']}\n",
    "\n",
    "Improved Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a fact-checking assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        improved = completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        improved = row[\"llm_response\"]  # fallback\n",
    "\n",
    "    revised_responses.append(improved)\n",
    "df_filtered = df.copy()\n",
    "df_filtered.loc[df_hallucinated.index, \"llm_response\"] = revised_responses\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "filtered_path = \"../outputs/filtered_responses.csv\"\n",
    "df_filtered[[\"question\", \"ground_truth\", \"llm_response\", \"trigger_type\"]].to_csv(filtered_path, index=False)\n",
    "print(f\"Filtered responses saved to: {filtered_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
